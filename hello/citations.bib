
@article{jin_graph_2020,
	title = {Graph {Structure} {Learning} for {Robust} {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2005.10203},
	abstract = {Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses (footnote: https://github.com/DSE-MSU/DeepRobust). The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.},
	urldate = {2021-02-26},
	journal = {arXiv:2005.10203 [cs, stat]},
	author = {Jin, Wei and Ma, Yao and Liu, Xiaorui and Tang, Xianfeng and Wang, Suhang and Tang, Jiliang},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.10203},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{dai_adversarial_2018,
	title = {Adversarial {Attack} on {Graph} {Structured} {Data}},
	url = {http://arxiv.org/abs/1806.02371},
	abstract = {Deep learning on graph structures has shown exciting results in various applications. However, few attentions have been paid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this paper, we focus on the adversarial attacks that fool the model by modifying the combinatorial structure of data. We first propose a reinforcement learning based attack method that learns the generalizable attack policy, while only requiring prediction labels from the target classifier. Also, variants of genetic algorithms and gradient methods are presented in the scenario where prediction confidence or gradients are available. We use both synthetic and real-world data to show that, a family of Graph Neural Network models are vulnerable to these attacks, in both graph-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.},
	urldate = {2021-02-26},
	journal = {arXiv:1806.02371 [cs, stat]},
	author = {Dai, Hanjun and Li, Hui and Tian, Tian and Huang, Xin and Wang, Lin and Zhu, Jun and Song, Le},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02371},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@article{zugner_adversarial_2018,
	title = {Adversarial {Attacks} on {Neural} {Networks} for {Graph} {Data}},
	url = {http://arxiv.org/abs/1805.07984},
	doi = {10.1145/3219819.3220078},
	abstract = {Deep learning models for graphs have achieved strong performance for the task of node classification. Despite their proliferation, currently there is no study of their robustness to adversarial attacks. Yet, in domains where they are likely to be used, e.g. the web, adversaries are common. Can deep learning models for graphs be easily fooled? In this work, we introduce the first study of adversarial attacks on attributed graphs, specifically focusing on models exploiting ideas of graph convolutions. In addition to attacks at test time, we tackle the more challenging class of poisoning/causative attacks, which focus on the training phase of a machine learning model. We generate adversarial perturbations targeting the node's features and the graph structure, thus, taking the dependencies between instances in account. Moreover, we ensure that the perturbations remain unnoticeable by preserving important data characteristics. To cope with the underlying discrete domain we propose an efficient algorithm Nettack exploiting incremental computations. Our experimental study shows that accuracy of node classification significantly drops even when performing only few perturbations. Even more, our attacks are transferable: the learned attacks generalize to other state-of-the-art node classification models and unsupervised approaches, and likewise are successful even when only limited knowledge about the graph is given.},
	urldate = {2021-02-26},
	journal = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	author = {Zügner, Daniel and Akbarnejad, Amir and Günnemann, Stephan},
	month = jul,
	year = {2018},
	note = {arXiv: 1805.07984},
	keywords = {Statistics - Machine Learning, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {2847--2856},
}

@inproceedings{zhu_robust_2019,
	address = {Anchorage, AK, USA},
	series = {{KDD} '19},
	title = {Robust {Graph} {Convolutional} {Networks} {Against} {Adversarial} {Attacks}},
	isbn = {9781450362016},
	url = {https://doi.org/10.1145/3292500.3330851},
	doi = {10.1145/3292500.3330851},
	abstract = {Graph Convolutional Networks (GCNs) are an emerging type of neural network model on graphs which have achieved state-of-the-art performance in the task of node classification. However, recent studies show that GCNs are vulnerable to adversarial attacks, i.e. small deliberate perturbations in graph structures and node attributes, which poses great challenges for applying GCNs to real world applications. How to enhance the robustness of GCNs remains a critical open problem. To address this problem, we propose Robust GCN (RGCN), a novel model that "fortifies'' GCNs against adversarial attacks. Specifically, instead of representing nodes as vectors, our method adopts Gaussian distributions as the hidden representations of nodes in each convolutional layer. In this way, when the graph is attacked, our model can automatically absorb the effects of adversarial changes in the variances of the Gaussian distributions. Moreover, to remedy the propagation of adversarial attacks in GCNs, we propose a variance-based attention mechanism, i.e. assigning different weights to node neighborhoods according to their variances when performing convolutions. Extensive experimental results demonstrate that our proposed method can effectively improve the robustness of GCNs. On three benchmark graphs, our RGCN consistently shows a substantial gain in node classification accuracy compared with state-of-the-art GCNs against various adversarial attack strategies.},
	urldate = {2021-02-25},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Zhu, Dingyuan and Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
	month = jul,
	year = {2019},
	keywords = {adversarial attacks, deep learning, graph convolutional networks, robustness},
	pages = {1399--1407},
}

@article{wu_adversarial_2019,
	title = {Adversarial {Examples} on {Graph} {Data}: {Deep} {Insights} into {Attack} and {Defense}},
	shorttitle = {Adversarial {Examples} on {Graph} {Data}},
	url = {http://arxiv.org/abs/1903.01610},
	abstract = {Graph deep learning models, such as graph convolutional networks (GCN) achieve remarkable performance for tasks on graph data. Similar to other types of deep models, graph deep learning models often suffer from adversarial attacks. However, compared with non-graph data, the discrete features, graph connections and different definitions of imperceptible perturbations bring unique challenges and opportunities for the adversarial attacks and defenses for graph data. In this paper, we propose both attack and defense techniques. For attack, we show that the discreteness problem could easily be resolved by introducing integrated gradients which could accurately reflect the effect of perturbing certain features or edges while still benefiting from the parallel computations. For defense, we observe that the adversarially manipulated graph for the targeted attack differs from normal graphs statistically. Based on this observation, we propose a defense approach which inspects the graph and recovers the potential adversarial perturbations. Our experiments on a number of datasets show the effectiveness of the proposed methods.},
	urldate = {2021-02-26},
	journal = {arXiv:1903.01610 [cs, stat]},
	author = {Wu, Huijun and Wang, Chen and Tyshetskiy, Yuriy and Docherty, Andrew and Lu, Kai and Zhu, Liming},
	month = may,
	year = {2019},
	note = {arXiv: 1903.01610},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security, Statistics - Machine Learning},
}
